# Design Document: Goal-Conditioned Manipulation Policy

**Objective:** Train a policy for the `MeatOnGrill` task that can reliably pick up a specified object (chicken or steak) and place it on the grill, starting with Behavioral Cloning and enabling future RL fine-tuning.

---

### 1. Code Structure

The project is organized in the `skill_training/` directory:
- `plan.txt`: This document.
- `train.py`: The main executable script for training and evaluation.
- `bc_agent.py`: Contains the agent and policy architecture definitions.

---

### 2. Phase 1: Goal-Conditioned Behavioral Cloning (BC)

This phase focuses on teaching the policy basic skills by imitating an expert in a supervised learning fashion.

#### 2.1. Data Generation
- **Method**: Demonstrations are generated "on the fly" at the start of the training script using RLBench's `live_demos=True` feature.
- **Dataset Size**: We generate 5 demonstrations for each of the 2 task variations, for a total of 10 full demonstrations.
- **Data Handling**: The raw, sequential demonstrations are processed by a custom `DemonstrationDataset` class (in `bc_agent.py`). This class flattens all trajectories into a single, non-sequential list of `(image, goal, action)` tuples.

#### 2.2. Model Architecture (`BCPolicy`)
- **Vision Encoder**: A pre-trained ResNet18 model from `torchvision`. The weights of the convolutional layers are frozen (`requires_grad=False`), so it acts as a fixed, high-quality feature extractor.
- **Goal Encoder**: The goal is a one-hot vector that is used directly without a separate encoder.
- **Fusion**: The 512-dimensional feature vector from the ResNet is concatenated with the one-hot goal vector.
- **Policy Head**: The combined feature vector is passed through a small MLP (256 -> 128 -> action_dim) with ReLU activations. The final layer uses a `Tanh` activation to bound the output actions between -1 and 1.

#### 2.3. Training Process (`BCAgent`)
- **Data Loading**: The `DemonstrationDataset` is wrapped in a PyTorch `DataLoader`. 
- **Shuffling**: The `DataLoader` is configured with `shuffle=True`. This is critical because it breaks the temporal correlations between consecutive frames in the demos. This ensures each training batch is I.I.D. (Independent and Identically Distributed), which is essential for stable supervised learning.
- **Optimizer**: We use the `Adam` optimizer (`torch.optim.Adam`), as it is a robust and effective choice for this type of training.
- **Loss Function**: A standard Mean Squared Error (`nn.MSELoss`) between the policy's predicted action and the expert's action.

--- 

### 3. Experiment Tracking & Monitoring

- **Tool**: We use **Weights & Biases (`wandb`)** to track experiments.
- **Logged Metrics**:
    - `training_loss`: The average training loss is logged at the end of each epoch.
    - `eval_success_var_X`: At the end of each evaluation episode, a success metric (1.0 for success, 0.0 for failure) is logged for the specific task variation that was tested.

---

### 4. Evaluation Protocol

- **Method**: Evaluation is performed "closed-loop" after the BC training phase is complete.
- **Data**: The evaluation runs on **new, unseen episodes**. The `task.reset()` function is called to generate a new, randomized scene. This is fundamentally different from the data used for training.
- **Purpose**: This method tests the policy's ability to **generalize** its learned skills to new situations, not just its ability to memorize the training set.

---

### 5. Phase 2: Reinforcement Learning (RL) Fine-Tuning (Future Work)

- **Initialization**: The RL agent's policy network (actor) will be architecturally identical to the `BCPolicy`.
- **Knowledge Transfer**: The training process will start by loading the saved weights from the BC phase (`bc_policy.pth`) into the actor network, providing a strong initial policy.